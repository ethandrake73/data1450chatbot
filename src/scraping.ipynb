{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install selenium\n",
    "# %pip install webdriver_manager\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Define options object that contains metadata on the browser. This can be customized in a number of ways to simulate certain traits \n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Uncomment to run without displaying a browser\n",
    "options.headless = True\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.set_window_size(1600, 1600)\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "wait = WebDriverWait(driver, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "def scrape_pdfs_and_download(url, download_folder):\n",
    "    # Initialize the Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the filings section to be loaded\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"filings\"))\n",
    "    )\n",
    "    \n",
    "    # Find all <a> elements within the filings section\n",
    "    pdf_links = driver.find_elements(By.CSS_SELECTOR, \"#filings a\")\n",
    "    \n",
    "    # Create the download folder if it doesn't exist\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    # Download each PDF file\n",
    "    for link in pdf_links:\n",
    "        pdf_url = link.get_attribute(\"href\")\n",
    "        pdf_filename = pdf_url.split(\"/\")[-1]\n",
    "        pdf_path = os.path.join(download_folder, pdf_filename)\n",
    "        \n",
    "        # Download the PDF file\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            response = requests.get(pdf_url)\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://eeaonline.eea.state.ma.us/DPU/Fileroom/dockets/bynumber/20-80\"\n",
    "download_folder = \"pdf_files\"\n",
    "scrape_pdfs_and_download(url, download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pdfs_and_download_test(url, download_folder):\n",
    "    # Initialize the Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the filings section to be loaded\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"filings\"))\n",
    "    )\n",
    "    \n",
    "    # Find all <a> elements within the filings section\n",
    "    pdf_links = driver.find_elements(By.CSS_SELECTOR, \"#filings a\")\n",
    "    \n",
    "    # Create the download folder if it doesn't exist\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    # Download only the first two PDF files\n",
    "    for i, link in enumerate(pdf_links[:2], 1):\n",
    "        pdf_url = link.get_attribute(\"href\")\n",
    "        pdf_filename = f\"pdf_{i}.pdf\"\n",
    "        pdf_path = os.path.join(download_folder, pdf_filename)\n",
    "        \n",
    "        # Download the PDF file\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            response = requests.get(pdf_url)\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://eeaonline.eea.state.ma.us/DPU/Fileroom/dockets/bynumber/20-80\"\n",
    "download_folder = \"pdf_files_test\"\n",
    "scrape_pdfs_and_download_test(url, download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pdfs_and_download(url, download_folder):\n",
    "    # Initialize the Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the filings section to be loaded\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"filings\"))\n",
    "    )\n",
    "    \n",
    "    # Find all <a> elements within the filings section\n",
    "    pdf_links = driver.find_elements(By.CSS_SELECTOR, \"#filings a\")\n",
    "    \n",
    "    # Create the download folder if it doesn't exist\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    # Download all PDF files\n",
    "    for i, link in enumerate(pdf_links, 1):\n",
    "        pdf_url = link.get_attribute(\"href\")\n",
    "        pdf_filename = f\"pdf_{i}.pdf\"\n",
    "        pdf_path = os.path.join(download_folder, pdf_filename)\n",
    "        \n",
    "        # Download the PDF file\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            response = requests.get(pdf_url)\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://eeaonline.eea.state.ma.us/DPU/Fileroom/dockets/bynumber/20-80\"\n",
    "download_folder = \"pdf_files\"\n",
    "scrape_pdfs_and_download(url, download_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
